{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of RNNs from scratch\n",
    "\n",
    "Now we will implement a language model based on a character-level RNN trained on H. G. Wells’ *The Time Machine*. We start by reading the data set first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import torch\n",
    "import math\n",
    "import d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, num_steps = 32, 35\n",
    "train_iter, vocab = utils.load_data_time_machine(batch_size,\n",
    "                                                num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = next(train_iter.get_iter())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 35])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot Encoding\n",
    "\n",
    "Remember that each token is presented as a numerical index in ``train_iter``. Feeding these indices directly to the neural network might make it hard to learn. We often present each token as a more expressive feature vector. The easiest presentation is called ``one-hot encoding``.\n",
    "\n",
    "In a nutshell, we map each index to a different unit vector: assume that the number of different tokens in the vocabulary is N (the ``len(vocab)``) and the token indices range from 0 to N-1. If the index of a token is the integer i , then we create a vector $\\mathbf{e}_i$ of all 0s with a length of N and set the element at position i to 1. This vector is the one-hot vector of the original token. The one-hot vectors with indices 0 and 2 are shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([0, 2])\n",
    "x_one_hot = torch.nn.functional.one_hot(x, len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0],\n",
       "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape of the mini-batch we sample each time is (batch size, time step). The ``one_hot`` function transforms such a mini-batch into a 3-D tensor with the last dimension equals to the vocabulary size. We often transpose the input so that we will obtain a (time step, batch size, vocabulary size) output that fits into a sequence\n",
    "model easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 2, 28])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# type your code here\n",
    "X = torch.arange(10).reshape(2, 5)\n",
    "torch.nn.functional.one_hot(X.T, len(vocab)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing the Model Parameters\n",
    "\n",
    "Next, we initialize the model parameters for a RNN model. The number of hidden units ``num_hiddens`` is a tunable parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params(vocab_size, num_hiddens):\n",
    "    num_inputs = num_outputs = vocab_size\n",
    "    # Hidden layer parameters\n",
    "    normal = lambda shape: 0.01*torch.randn(size=shape)\n",
    "    W_xh = normal((num_inputs, num_hiddens))\n",
    "    W_hh = normal((num_hiddens, num_hiddens))\n",
    "    b_h = torch.zeros(num_hiddens)\n",
    "    # Output layer parameters\n",
    "    W_hq = normal((num_hiddens, num_outputs))\n",
    "    b_q = torch.zeros(num_outputs)\n",
    "    # Attach a gradient\n",
    "    params = [W_xh, W_hh, b_h, W_hq, b_q]\n",
    "    for param in params: \n",
    "        param.requires_grad = True\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = get_params(28, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([28, 3])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Model\n",
    "\n",
    "First, we need an ``init_rnn_state`` function to return the hidden state at initialization. It returns a tuple consisting of a tensor with a value of 0 and a shape of (batch size, number of hidden units). Using tuples makes it easier to handle situations where the hidden state contains multiple variables (e.g. when combining multiple layers in an RNN where each layers requires initializing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_rnn_state(batch_size, num_hiddens):\n",
    "    return (torch.zeros(size=(batch_size, num_hiddens)), )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following ``rnn`` function defines how to compute the hidden state and output in a time step. The activation function here uses the ``tanh`` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn(inputs, state, params):\n",
    "    # inputs shape: (num_steps, batch_size, vocab_size)\n",
    "    W_xh, W_hh, b_h, W_hq, b_q = params\n",
    "    H, = state\n",
    "    # type your code here\n",
    "    outputs = []\n",
    "    for X in inputs:\n",
    "        H = torch.tanh( torch.matmul(X.type(torch.FloatTensor), W_xh) + \n",
    "                      torch.matmul(H, W_hh) + b_h)\n",
    "        Y = torch.matmul(H, W_hq) + b_q\n",
    "        outputs.append(Y)\n",
    "    return torch.cat(outputs, dim=0), (H, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have all functions defined, next we create a class to wrap these functions and store parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModelScratch(object):\n",
    "    \"\"\"A RNN Model based on scratch implementations\"\"\"\n",
    "    def __init__(self, vocab_size, num_hiddens, get_params, init_state, forward):\n",
    "        self.vocab_size, self.num_hiddens = vocab_size, num_hiddens\n",
    "        self.params = get_params(vocab_size, num_hiddens)\n",
    "        self.init_state, self.forward_fn = init_state, forward\n",
    "    def __call__(self, X, state):\n",
    "        X = torch.nn.functional.one_hot(X.T, self.vocab_size)\n",
    "        return self.forward_fn(X, state, self.params)\n",
    "    def begin_state(self, batch_size):\n",
    "        return self.init_state(batch_size, self.num_hiddens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s do a sanity check whether inputs and outputs have the correct dimensions, e.g. to ensure that the dimensionality of the hidden state hasn’t changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2, 3, 4],\n",
       "        [5, 6, 7, 8, 9]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 28]), 1, torch.Size([2, 512]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size, num_hiddens = len(vocab), 512\n",
    "model = RNNModelScratch(vocab_size, num_hiddens, get_params,\n",
    "                        init_rnn_state, rnn)\n",
    "state = model.begin_state(X.shape[0])\n",
    "Y, new_state = model(X, state)\n",
    "Y.shape, len(new_state), new_state[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the output shape is (number steps x batch size, vocabulary size), while the state shape remains the same, i.e. (batch size, number of hidden units)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first explain the predicting function so we can regularly check the prediction during training. This function predicts the next ``num_predicts`` characters based on the prefix (a string containing several characters). For the beginning of the sequence, we only update the hidden state. After that we begin generating new characters and emitting them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rnn(prefix, num_predicts, model, vocab):\n",
    "    # type your code here\n",
    "    for y in prefix[1:]: # Warmup state with prefix\n",
    "        # type your code here\n",
    "    for _ in range(num_predicts): # Predict num_predicts steps\n",
    "        # type your code here\n",
    "    return # type your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We test the ``predict_rnn`` function first. Given that we didn’t train the network it will generate nonsensical predictions. We initialize it with the sequence traveller and have it generate 10 additional characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Clipping\n",
    "\n",
    "For a sequence of length T, we compute the gradients over these T time steps in an iteration, which results in a chain of matrix-products with length $O(T)$ during backpropagating. As we mentioned before, it might result in numerical instability, e.g. the gradients may either explode or vanish, when T is large. Therefore RNN models often need extra help to stabilize the training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes the gradients can be quite large and the optimization algorithm may fail to converge. We could address this by reducing the learning rate $\\eta$ or by some other higher order trick. But what if we only rarely get large gradients? In this case such an approach may appear entirely unwarranted. One alternative is to\n",
    "clip the gradients by projecting them back to a ball of a given radius, say $\\theta$ via $$\\mathbf{g} \\leftarrow \\min\\left(1, \\frac{\\theta}{\\|\\mathbf{g} \\|} \\right) \\mathbf{g}$$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By doing so we know that the gradient norm never exceeds $\\theta$ and that the updated gradient is entirely aligned with the original direction $\\mathbf{g}$. This bestows a certain degree of robustness to the model. Gradient clipping provides a quick fix to the gradient exploding. While it doesn’t entirely solve the problem, it is one of the many techniques to alleviate it.\n",
    "\n",
    "Below we define a function to clip the gradients of a model. Note that we compute the gradient norm over all parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_clipping(model, theta):\n",
    "    # type your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Let’s first define the function to train the model on one data epoch. It differs to the\n",
    "models training from previous chapters in three places:\n",
    "\n",
    "* Different sampling methods for sequential data (random sampling and sequential partitioning) will result in differences in the initialization of hidden states.\n",
    "\n",
    "* We clip the gradient before updating the model parameters. This ensures that the model doesn’t diverge even when gradients blow up at some point during the training process (effectively it reduces the stepsize automatically).\n",
    "\n",
    "* We use perplexity to evaluate the model. This ensures that different tests are comparable.\n",
    "\n",
    "When the consecutive sampling is used, we initialize the hidden state at the beginning of each epoch. Since the i-th example in the next mini-batch is adjacent to the current i-th example, so the next mini-batch can use the current hidden state directly, we only detach the gradient so that we only compute the gradients within a mini-batch. When using the random sampling, we need to re-initialize the hidden state for each iteration since each example is sampled with a random position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch_rnn(model, train_iter, loss, updater, use_random_iter):\n",
    "    # type your code here\n",
    "    for X, Y in train_iter:\n",
    "        if state is None or use_random_iter:\n",
    "            # Initialize state when either it's the first iteration or\n",
    "            # using random sampling.\n",
    "            state = model.begin_state(batch_size=X.shape[0])\n",
    "    else:\n",
    "        for s in state: \n",
    "            # type your code here\n",
    "    # type your code here\n",
    "    return # type your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rnn(model, train_iter, vocab, lr, num_epochs, use_random_iter=False):\n",
    "    # type your code here\n",
    "    # Train and check the progress.\n",
    "    for epoch in range(num_epochs):\n",
    "        # type your code here\n",
    "    # type your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can train a model. Since we only use 10,000 tokens in the dataset, so here we need more data epochs to converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
