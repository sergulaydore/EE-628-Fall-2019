{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concise Implementation of Softmax Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as PyTOrch made it much easier to implement linear regression in notebook ``concise_implementation_of_linear_regression``, we will find it similarly convenient for implementing classification models.\n",
    "\n",
    "Let's import tha necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "from torch import nn, optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from IPython import display\n",
    "import d2l\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s stick with the Fashion-MNIST dataset and keep the batch size at 256."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_fashion_mnist(batch_size, resize=None, num_workers=0):\n",
    "    tranform_list = []\n",
    "    if resize:\n",
    "        tranform_list.append(torchvision.transforms.Resize(resize))\n",
    "    tranform_list.append(transforms.ToTensor())\n",
    "    transform = transforms.Compose(tranform_list)\n",
    "    mnist_train = datasets.FashionMNIST('~/datasets/F_MNIST/',\n",
    "                                 download=True,\n",
    "                                 train=True,\n",
    "                                 transform=transform)\n",
    "\n",
    "    mnist_test = datasets.FashionMNIST('~/datasets/F_MNIST/',\n",
    "                                     download=True,\n",
    "                                     train=False,\n",
    "                                    transform=transform)\n",
    "    \n",
    "    train_iter = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, \n",
    "                                          shuffle=True, num_workers=num_workers)\n",
    "    test_iter = torch.utils.data.DataLoader(mnist_test, batch_size=batch_size, \n",
    "                                              shuffle=False)\n",
    "    return train_iter, test_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "\n",
    "train_iter, test_iter = # insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = # insert your code here\n",
    "output_dim = # insert your code here\n",
    "net = # insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In notebook ``implementation_of_softmax_from_scratch``, we calculated our model’s output and then ran this output through the cross-entropy\n",
    "loss. At its heart it uses ``torch.mean(-torch.log(y_hat.gather(1, y.view(-1, 1))))``. Mathematically, that’s a perfectly reasonable thing\n",
    "to do. However, computationally, things can get hairy when dealing with exponentiation due to numerical\n",
    "stability issues (e.g. in Section 4.5). Recall that the softmax function calculates $\\hat{y}_j = \\frac{e^{z_j}}{\\sum_{i=1}^n e^{z_i}}$, where $\\hat{y}_j$ is the j-th element\n",
    "of $\\hat{y}$ and $z_j$ is the j-th element of the input ``y_linear`` variable, as computed by the softmax."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our salvation is that even though we’re computing these exponential functions, we ultimately plan to take\n",
    "their log in the cross-entropy functions. It turns out that by combining these two operators softmax and\n",
    "cross_entropy together, we can escape the numerical stability issues that might otherwise plague us during\n",
    "backpropagation. As shown in the equation below, we avoided calculating $e^{z_j}$ but directly using $z_j$ due to\n",
    "$log(exp())$.\n",
    "\n",
    "$$\\log (\\hat{y}_j) = \\log \\left(\\frac{e^{z_j}}{\\sum_{i=1}^n e^{z_i}} \\right) = z_j - \\log \\left(\\sum_{i=1}^n e^{z_i} \\right)$$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch already implements this trick in its [``CrossEntropyLoss``](https://pytorch.org/docs/0.3.1/nn.html?highlight=crossentropyloss#torch.nn.CrossEntropyLoss)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = # insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the mini-batch random gradient descent with a learning rate of 0.1 as the optimization algorithm.\n",
    "Note that this is the same choice as for linear regression and it illustrates the general applicability of the\n",
    "optimizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.1\n",
    "optimizer = # insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Next, we use the training functions defined in notebook ``implementation_of_softmax_regression_from_scratch`` to train a model. Note that we made a slight modification in ``train_epoch`` and ``evaluate_accuracy`` functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_hat, y):\n",
    "    return (y_hat.argmax(dim=1) == y).sum()\n",
    "\n",
    "class Accumulator(object):\n",
    "    \"\"\"Sum a list of numbers over time\"\"\"\n",
    "    def __init__(self, n):\n",
    "        self.data = [0.0] * n\n",
    "    def add(self, *args):\n",
    "        self.data = [a+b for a, b in zip(self.data, args)]\n",
    "    def reset(self):\n",
    "        self.data = [0] * len(self.data)\n",
    "    def __getitem__(self, i):\n",
    "        return self.data[i]\n",
    "    \n",
    "def evaluate_accuracy(net, data_iter):\n",
    "    metric = Accumulator(2)\n",
    "    for X, y in data_iter:\n",
    "        metric.add(accuracy(net(X.view(-1, X.shape[2]*X.shape[3])), y), len(y))\n",
    "    return float(metric[0])/metric[1]\n",
    "\n",
    "def train_epoch(net, train_iter, loss, optimizer):\n",
    "    metric = Accumulator(3) # train_loss_sum, train_acc_sum, num_examples\n",
    "    for X, y in train_iter:\n",
    "        y_hat = net(X.view(-1, X.shape[2]*X.shape[3]))\n",
    "        # compute gradients and update parameters\n",
    "        l = loss(y_hat, y)\n",
    "        optimizer.zero_grad()\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "        l_sum = float(l)*len(y)\n",
    "        metric.add(l_sum, float(accuracy(y_hat, y)), len(y))\n",
    "    return metric[0]/metric[2], metric[1]/metric[2]\n",
    "\n",
    "class Animator(object):\n",
    "    def __init__(self, xlabel=None, ylabel=None, legend=[], xlim=None,\n",
    "                 ylim=None, xscale='linear', yscale='linear', fmts=None,\n",
    "                 nrows=1, ncols=1, figsize=(3.5, 2.5)):\n",
    "        \"\"\"Incrementally plot multiple lines.\"\"\"\n",
    "        d2l.use_svg_display()\n",
    "        self.fig, self.axes = d2l.plt.subplots(nrows, ncols, figsize=figsize)\n",
    "        if nrows * ncols == 1: self.axes = [self.axes,]\n",
    "        # use a lambda to capture arguments\n",
    "        self.config_axes = lambda : d2l.set_axes(self.axes[0], xlabel, ylabel,\n",
    "                                                 xlim, ylim, xscale, yscale, legend)\n",
    "        self.X, self.Y, self.fmts = None, None, fmts\n",
    "    def add(self, x, y):\n",
    "        \"\"\"Add multiple data points into the figure.\"\"\"\n",
    "        if not hasattr(y, \"__len__\"): y = [y]\n",
    "        n = len(y)\n",
    "        if not hasattr(x, \"__len__\"): x = [x] * n\n",
    "        if not self.X: self.X = [[] for _ in range(n)]\n",
    "        if not self.Y: self.Y = [[] for _ in range(n)]\n",
    "        if not self.fmts: self.fmts = ['-'] * n\n",
    "        for i, (a, b) in enumerate(zip(x, y)):\n",
    "            if a is not None and b is not None:\n",
    "                self.X[i].append(a)\n",
    "                self.Y[i].append(b)\n",
    "        self.axes[0].cla()\n",
    "        for x, y, fmt in zip(self.X, self.Y, self.fmts):\n",
    "            self.axes[0].plot(x, y, fmt)\n",
    "        self.config_axes()\n",
    "        display.display(self.fig)\n",
    "        display.clear_output(wait=True)\n",
    "        \n",
    "def train(net, train_iter, test_iter, loss, num_epochs, updater):\n",
    "    trains, test_accs = [], []\n",
    "    animator = Animator(xlabel='epoch', xlim=[1, num_epochs], \n",
    "                        ylim=[0.3, 0.9], legend=['train loss', 'train acc', 'test acc'])\n",
    "    for epoch in range(num_epochs):\n",
    "        train_metrics = train_epoch(net, train_iter, loss, updater)\n",
    "        test_acc = evaluate_accuracy(net, test_iter)\n",
    "        animator.add(epoch+1, train_metrics+(test_acc,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "# insert your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
