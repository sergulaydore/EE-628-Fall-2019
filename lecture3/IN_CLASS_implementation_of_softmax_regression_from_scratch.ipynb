{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of Softmax Regression from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as we implemented linear regression from scratch, we believe that multiclass logistic (softmax) regression\n",
    "is similarly fundamental and you ought to know the gory details of how to implement it from scratch. As\n",
    "with linear regression, after doing things by hand we will breeze through an implementation in PyTorch for\n",
    "comparison. To begin, let’s import our packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import d2l\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will work with the Fashion-MNIST dataset just introduced, cuing up an iterator with batch size 256."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_fashion_mnist(batch_size, resize=None, num_workers=0):\n",
    "    tranform_list = []\n",
    "    if resize:\n",
    "        tranform_list.append(torchvision.transforms.Resize(resize))\n",
    "    tranform_list.append(transforms.ToTensor())\n",
    "    transform = transforms.Compose(tranform_list)\n",
    "    mnist_train = datasets.FashionMNIST('~/datasets/F_MNIST/',\n",
    "                                 download=True,\n",
    "                                 train=True,\n",
    "                                 transform=transform)\n",
    "\n",
    "    mnist_test = datasets.FashionMNIST('~/datasets/F_MNIST/',\n",
    "                                     download=True,\n",
    "                                     train=False,\n",
    "                                    transform=transform)\n",
    "    \n",
    "    train_iter = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, \n",
    "                                          shuffle=True, num_workers=num_workers)\n",
    "    test_iter = torch.utils.data.DataLoader(mnist_test, batch_size=batch_size, \n",
    "                                              shuffle=False)\n",
    "    return train_iter, test_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "train_iter, test_iter = # insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Model Parameters\n",
    "\n",
    "Just as in linear regression, we represent each example as a vector. Since each example is a 28 x 28 image,\n",
    "we can flatten each example, treating them as 784 dimensional vectors. In the future, we’ll talk about more\n",
    "sophisticated strategies for exploiting the spatial structure in images, but for now we treat each pixel location\n",
    "as just another feature.\n",
    "\n",
    "Recall that in softmax regression, we have as many outputs as there are categories. Because our dataset has\n",
    "10 categories, our network will have an output dimension of 10. Consequently, our weights will constitute a\n",
    "784 x 10 matrix and the biases will constitute a 1 x 10 vector. As with linear regression, we will initialize\n",
    "our weights W with Gaussian noise and our biases to take the initial value 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs = # insert your code here\n",
    "num_outputs = # insert your code here\n",
    "\n",
    "W = # insert your code here\n",
    "b = # insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that we need to attach gradients to the model parameters. More literally, we are allocating memory\n",
    "for future gradients to be stored and notifiying PyTorch that we want gradients to be calculated with respect\n",
    "to these parameters in the first place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before implementing the softmax regression model, let’s briefly review how operators such as sum work along\n",
    "specific dimensions in a tensor. Given a matrix X we can sum over all elements (default) or only over\n",
    "elements in the same column (axis=0) or the same row (axis=1). \n",
    "\n",
    "Note that if X is an array with shape (2, 3) and we sum over the columns (X.sum(axis=0), the result will be a (1D) vector with shape (3,). If we\n",
    "want to keep the number of axes in the original array (resulting in a 2D array with shape (1,3)), rather\n",
    "than collapsing out the dimension that we summed over we can specify keepdims=True when invoking sum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor([[1, 2, 3], [4, 5, 6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# illustrate using dim=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# illustrate using dim=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# illustrate using dim=1 and keepdims"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to implement the softmax function. Recall that softmax consists of two steps: First, we\n",
    "exponentiate each term (using exp). Then, we sum over each row (we have one row per example in the\n",
    "batch) to get the normalization constants for each example. Finally, we divide each row by its normalization\n",
    "constant, ensuring that the result sums to 1. Before looking at the code, let’s recall what this looks expressed\n",
    "as an equation: $$\\mbox{softmax}(\\mathbf{X})_{ij} = \\frac{\\exp(X_{ij})}{\\sum_k \\exp(X_{ik})}$$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X):\n",
    "    # insert your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# illustrate using 2 x 3 matrix and show columns sum up to 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have defined the softmax operation, we can implement the softmax regression model. The\n",
    "below code defines the forward pass through the network. Note that we flatten each original image in the\n",
    "batch into a vector with length num_inputs with the reshape function before passing the data through our\n",
    "model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def net(X):\n",
    "    # insert your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# illustrate net with 10, 28, 28 random vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to implement the cross-entropy loss function, introduced in the class. This may be the most\n",
    "common loss function in all of deep learning because, at the moment, classification problems far outnumber\n",
    "regression problems.\n",
    "\n",
    "Recall that cross-entropy takes the negative log likelihood of the predicted probability assigned to the true\n",
    "label $\\log p(y|x)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(y_hat, y):\n",
    "    # insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the predicted probability distribution ``y_hat``, we typically choose the class with highest predicted\n",
    "probability whenever we must output a hard prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_hat, y):\n",
    "    # insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we define ``Accumulator`` is a utility class to accumulate sum over multiple numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accumulator(object):\n",
    "    \"\"\"Sum a list of numbers over time\"\"\"\n",
    "    def __init__(self, n):\n",
    "        self.data = [0.0] * n\n",
    "    def add(self, *args):\n",
    "        self.data = [a+b for a, b in zip(self.data, args)]\n",
    "    def reset(self):\n",
    "        self.data = [0] * len(self.data)\n",
    "    def __getitem__(self, i):\n",
    "        return self.data[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can evaluate the accuracy for model net on the data set (accessed via data_iter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(net, data_iter):\n",
    "    metric = Accumulator(2)\n",
    "    for X, y in data_iter:\n",
    "        metric.add(accuracy(net(X), y), len(y))\n",
    "    return float(metric[0])/metric[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# illustrate evaluate_accuracy on our test_iter and net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training loop for softmax regression should look strikingly familiar to our implementation\n",
    "of linear regression. Here we refactor the implementation to make it reusable. First,\n",
    "we define a function to train for one data epoch. Note that updater is general function to update the model\n",
    "parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's grab our sgd function we used in linear regression from scratch notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(params, lr):\n",
    "    # insert your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(net, train_iter, loss, updater):\n",
    "    metric = Accumulator(3) # train_loss_sum, train_acc_sum, num_examples\n",
    "    # insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before showing the implementation of the training function, we define a utility class that draw data in\n",
    "animation. Again, it aims to simplify the codes in later chapters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Animator(object):\n",
    "    def __init__(self, xlabel=None, ylabel=None, legend=[], xlim=None,\n",
    "                 ylim=None, xscale='linear', yscale='linear', fmts=None,\n",
    "                 nrows=1, ncols=1, figsize=(3.5, 2.5)):\n",
    "        \"\"\"Incrementally plot multiple lines.\"\"\"\n",
    "        d2l.use_svg_display()\n",
    "        self.fig, self.axes = d2l.plt.subplots(nrows, ncols, figsize=figsize)\n",
    "        if nrows * ncols == 1: self.axes = [self.axes,]\n",
    "        # use a lambda to capture arguments\n",
    "        self.config_axes = lambda : d2l.set_axes(self.axes[0], xlabel, ylabel,\n",
    "                                                 xlim, ylim, xscale, yscale, legend)\n",
    "        self.X, self.Y, self.fmts = None, None, fmts\n",
    "    def add(self, x, y):\n",
    "        \"\"\"Add multiple data points into the figure.\"\"\"\n",
    "        if not hasattr(y, \"__len__\"): y = [y]\n",
    "        n = len(y)\n",
    "        if not hasattr(x, \"__len__\"): x = [x] * n\n",
    "        if not self.X: self.X = [[] for _ in range(n)]\n",
    "        if not self.Y: self.Y = [[] for _ in range(n)]\n",
    "        if not self.fmts: self.fmts = ['-'] * n\n",
    "        for i, (a, b) in enumerate(zip(x, y)):\n",
    "            if a is not None and b is not None:\n",
    "                self.X[i].append(a)\n",
    "                self.Y[i].append(b)\n",
    "        self.axes[0].cla()\n",
    "        for x, y, fmt in zip(self.X, self.Y, self.fmts):\n",
    "            self.axes[0].plot(x, y, fmt)\n",
    "        self.config_axes()\n",
    "        display.display(self.fig)\n",
    "        display.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training function then runs multiple epochs and visualize the training progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, train_iter, test_iter, loss, num_epochs, updater):\n",
    "    trains, test_accs = [], []\n",
    "    animator = Animator(xlabel='epoch', xlim=[1, num_epochs], \n",
    "                        ylim=[0.3, 0.9], legend=['train loss', 'train acc', 'test acc'])\n",
    "    for # insert your code here\n",
    "        train_metrics = # insert your code here\n",
    "        test_acc = # insert your code here\n",
    "        animator.add(epoch+1, train_metrics+(test_acc,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we use the mini-batch stochastic gradient descent to optimize the loss function of the model. Note\n",
    "that the number of epochs (num_epochs), and learning rate (lr) are both adjustable hyper-parameters. By\n",
    "changing their values, we may be able to increase the classification accuracy of the model. In practice we’ll\n",
    "want to split our data three ways into training, validation, and test data, using the validation data to choose\n",
    "the best values of our hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "lr = 0.1\n",
    "updater = lambda: sgd([W, b], lr)\n",
    "\n",
    "train(net, train_iter, test_iter, cross_entropy, num_epochs, updater)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "\n",
    "Now that training is complete, our model is ready to classify some images. Given a series of images, we will\n",
    "compare their actual labels (first line of text output) and the model predictions (second line of text output)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's grab our functions for visualization and Fashion-MNIST labels from the notebook ``image_classification_data_Fashion-MNIST``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taken from d2l but modified to fit to PyTorch\n",
    "def show_images(imgs, num_rows, num_cols, titles=None, scale=1.5):\n",
    "    \"\"\"Plot a list of images.\"\"\"\n",
    "    figsize = (num_cols * scale, num_rows * scale)\n",
    "    _, axes = d2l.plt.subplots(num_rows, num_cols, figsize=figsize)\n",
    "    axes = axes.flatten()\n",
    "    for i, (ax, img) in enumerate(zip(axes, imgs)):\n",
    "        ax.imshow(img.squeeze(0).numpy())\n",
    "        ax.axes.get_xaxis().set_visible(False)\n",
    "        ax.axes.get_yaxis().set_visible(False)\n",
    "        if titles:\n",
    "            ax.set_title(titles[i])\n",
    "            \n",
    "def get_fashion_mnist_labels(labels):\n",
    "    text_labels = ['t-shirt', 'trouser', 'pullover', 'dress', 'coat',\n",
    "                  'sandal', 'shirt', 'sneaker', 'bag', 'ankle_boot']\n",
    "    return [text_labels[int(i)] for i in labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(net, test_iter, n=6):\n",
    "    for X, y in test_iter:\n",
    "        break\n",
    "    trues = get_fashion_mnist_labels(y.numpy())\n",
    "    preds = get_fashion_mnist_labels(net(X).argmax(axis=1).numpy())\n",
    "    titles = [true+'\\n'+ pred for true, pred in zip(trues, preds)]\n",
    "    show_images(X[0:n].reshape((n,28,28)), 1, n, titles=titles[0:n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(net, test_iter, 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "With softmax regression, we can train models for multi-category classification. The training loop is very\n",
    "similar to that in linear regression: retrieve and read data, define models and loss functions, then train\n",
    "models using optimization algorithms. As you’ll soon find out, most common deep learning models have\n",
    "similar training procedures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
