{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because they are so fundamental to deep learning, before going further, let’s take a brief look at some\n",
    "common activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReLU Function\n",
    "\n",
    "The most popular choice, due to its simplicity of implementation and its efficacy in training\n",
    "is the rectified linear unit (ReLU). ReLUs provide a very simple nonlinear transformation. Given the element\n",
    "$z$, the function is defined as the maximum of that element and 0. $$ReLU(z) = \\max(z, 0)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a better idea of what it looks like, we can plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create x from -8 to 8 with 0.1 steps and make requires_grad True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because it is used so commonly, PyTorch tensor supports the relu function as a basic native operator. As you can\n",
    "see, the activation function is piece-wise linear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use pytorch's relu on x to define y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x.detach().numpy(), y.detach().numpy())\n",
    "plt.grid(True)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('relu(x)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the input is negative, the derivative of ReLU function is 0 and when the input is positive, the derivative\n",
    "of ReLU function is 1. Note that the ReLU function is not differentiable when the input takes value precisely\n",
    "equal to 0. In these cases, we go with the left-hand-side (LHS) derivative and say that the derivative is 0\n",
    "when the input is 0. We can get away with this because the input may never actually be zero. There’s an old\n",
    "adage that if subtle boundary conditions matter, we are probably doing (real) mathematics, not engineering.\n",
    "That conventional wisdom may apply here. See the derivative of the ReLU function plotted below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward(torch.ones_like(x))\n",
    "# why do we need an input to backward function https://towardsdatascience.com/pytorch-autograd-understanding-the-heart-of-pytorchs-magic-2686cd94ec95\n",
    "\n",
    "plt.plot(x.detach().numpy(), x.grad.detach().numpy())\n",
    "plt.grid(True)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('grad of relu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason for using the ReLU is that its derivatives are particularly well behaved - either they vanish or\n",
    "they just let the argument through. This makes optimization better behaved and it reduces the issue of the\n",
    "vanishing gradient problem (more on this later)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sigmoid function transforms its inputs which take values in $\\mathbb{R}$ to the interval (0, 1). For that reason, the\n",
    "sigmoid is often called a *squashing* function: it squashes any input in the range (-inf, inf) to some value in\n",
    "the range (0,1). $$\\mbox{sigmoid}(x) = \\frac{1}{1 + \\exp (-x)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When attention shifted to gradient based learning, the sigmoid function was a natural choice because it\n",
    "is a smooth, differentiable approximation to a thresholding unit. Sigmoids are still common as activation\n",
    "functions on the output units, when we want to interpret the outputs as probabilities for binary classification\n",
    "problems (you can think of the sigmoid as a special case of the softmax) but the sigmoid has mostly been\n",
    "replaced by the simpler and easier to train ReLU for most use in hidden layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the sigmoid function plotted below. When the input is close to 0, the sigmoid function approaches a\n",
    "linear transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert your code here\n",
    "\n",
    "plt.plot(x.detach().numpy(), y.detach().numpy())\n",
    "plt.grid(True)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('sigmoid(x)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute the derivative of sigmoid function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{d}{dx}\\mbox{sigmoid}(x) = \\frac{\\exp(-x)}{(1 + \\exp(-x))^2} = \\mbox{sigmoid}(x)(1 - \\mbox{sigmoid}(x))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The derivative of sigmoid function is plotted below. Note that when the input is 0, the derivative of the\n",
    "sigmoid function reaches a maximum of 0.25. As the input diverges from 0 in either direction, the derivative\n",
    "approaches 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert your code here\n",
    "# why do we need an input to backward function https://towardsdatascience.com/pytorch-autograd-understanding-the-heart-of-pytorchs-magic-2686cd94ec95\n",
    "\n",
    "plt.plot(x.detach().numpy(), x.grad.detach().numpy())\n",
    "plt.grid(True)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('grad of sigmoid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tanh Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like the sigmoid function, the tanh (Hyperbolic Tangent) function also squashes its inputs, transforms them\n",
    "into elements on the interval between -1 and 1:\n",
    "$$tanh(x) = \\frac{1 - \\exp(-2x)}{1 + \\exp(-2x)} $$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the tanh function blow. Note that as the input nears 0, the tanh function approaches a linear\n",
    "transformation. Although the shape of the function is similar to the sigmoid function, the tanh function\n",
    "exhibits point symmetry about the origin of the coordinate system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert your code here\n",
    "\n",
    "\n",
    "plt.plot(x.detach().numpy(), y.detach().numpy())\n",
    "plt.grid(True)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('tanh(x)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The derivative of the Tanh function is: $$\\frac{d}{dx} tanh(x) = 1 - tanh^2(x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The derivative of tanh function is plotted below. As the input nears 0, the derivative of the tanh function\n",
    "approaches a maximum of 1. And as we saw with the sigmoid function, as the input moves away from 0 in\n",
    "either direction, the derivative of the tanh function approaches 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert your code here\n",
    "# why do we need an input to backward function https://towardsdatascience.com/pytorch-autograd-understanding-the-heart-of-pytorchs-magic-2686cd94ec95\n",
    "\n",
    "plt.plot(x.detach().numpy(), x.grad.detach().numpy())\n",
    "plt.grid(True)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('grad of tanh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary, we now know how to incorporate nonlinearities to build expressive multilayer neural network\n",
    "architectures. As a side note, your knowledge now already puts you in command of the state of the art in\n",
    "deep learning, circa 1990. In fact, you have an advantage over anyone working the 1990s, because you can\n",
    "leverage powerful open-source deep learning frameworks to build models rapidly, using only a few lines of code. Previously, getting these nets training required researchers to code up thousands of lines of C and\n",
    "Fortran."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
