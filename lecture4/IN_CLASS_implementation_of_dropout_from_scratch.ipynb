{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of Dropout from Scratch\n",
    "\n",
    "To implement the dropout function for a single layer, we must draw as many samples from a Bernoulli\n",
    "(binary) random variable as our layer has dimensions, where the random variable takes value 1 (keep) with\n",
    "probability 1-p and 0 (drop) with probability p. One easy way to implement this is to first draw samples\n",
    "from the uniform distribution U[0, 1]. Then we can keep those nodes for which the corresponding sample is\n",
    "greater than p, dropping the rest.\n",
    "\n",
    "In the following code, we implement a dropout function that drops out the elements in the tensor input\n",
    "X with probability ``drop_prob``, rescaling the remainder as described above (dividing the survivors by 1.\n",
    "0-drop_prob)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn, optim\n",
    "import numpy as np\n",
    "import d2l\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout(X, drop_prob):\n",
    "    # insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test out the dropout function on a few examples. In the following lines of code, we pass our input\n",
    "X through the dropout operation, with probabilities 0, 0.5, and 1, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.arange(start=0, end=16, \n",
    "                 out=torch.FloatTensor()).reshape(2, 8)\n",
    "# test your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model\n",
    "\n",
    "Again, we can use the Fashion-MNIST dataset. We will define a multilayer\n",
    "perceptron with two hidden layers. The two hidden layers both have 256 outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThreeLayerNet(torch.nn.Module):\n",
    "    def __init__(self, num_inputs, num_hiddens1, \n",
    "                 num_hiddens2, num_outputs):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate two nn.Linear modules and assign them as\n",
    "        member variables.\n",
    "        \"\"\"\n",
    "        super(ThreeLayerNet, self).__init__()\n",
    "        self.num_inputs = num_inputs\n",
    "        self.linear1 = torch.nn.Linear(num_inputs, num_hiddens1)\n",
    "        self.linear2 = torch.nn.Linear(num_hiddens1, num_hiddens2)\n",
    "        self.linear3 = torch.nn.Linear(num_hiddens2, num_outputs)\n",
    "        self.nonlinear_func = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Tensor of input data and we must return\n",
    "        a Tensor of output data. We can use Modules defined in the constructor as\n",
    "        well as arbitrary (differentiable) operations on Tensors.\n",
    "        \"\"\"\n",
    "        h_relu1 = self.nonlinear_func(self.linear1(x.reshape(-1, self.num_inputs)))\n",
    "        # insert your coder here\n",
    "        h_relu2 = self.nonlinear_func(self.linear2(h_relu1))\n",
    "        # insert your coder here\n",
    "        y_pred = self.linear2(h_relu2)\n",
    "        return y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = ThreeLayerNet(num_inputs=784, num_hiddens1=256,\n",
    "                  num_hiddens2=256, num_outputs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_prob1, drop_prob2 = 0.2, 0.5\n",
    "batch_size = 256\n",
    "train_iter, test_iter = utils.load_data_fashion_mnist(batch_size)\n",
    "num_epochs, lr = 10, 0.5\n",
    "optimizer = optim.SGD(net.parameters(), lr=lr)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "utils.train(net, train_iter, test_iter, loss, num_epochs, \n",
    "            optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.predict(net, test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
