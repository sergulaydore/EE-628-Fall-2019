{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading\n",
    "\n",
    "To get started we load text frim H. G. Wells' [Time Machine](http://www.gutenberg.org/ebooks/35). This is fairly small corpus of just over 30,000 words but for the purpose of what we want to illustrate this is just fine. More realistic document collection contain many billions of words. The following function read the dataset into a list of sentences, each sentence is a string. Here we ignore punctuation and capitalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import re\n",
    "\n",
    "def read_time_machine():\n",
    "    \"\"\"Load the time machine book into a list of sentences.\"\"\"\n",
    "    # type your code here\n",
    "# type your code here\n",
    "'# sentences %d' % len(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "\n",
    "For each sentence, we split it into a list of tokens. A token is a data point the model will train and predict. The following function support split a sentence into words or characters, and return a list of split sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(lines, token='word'):\n",
    "    \"\"\"Split sentences into word or char tokens\"\"\"\n",
    "    if token == 'word':\n",
    "        # type your code here\n",
    "    elif token == 'char':\n",
    "        # type your code here\n",
    "    else:\n",
    "        print('ERROR: unkown token type '+token)\n",
    "tokens = tokenize(lines)\n",
    "tokens[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocabulary\n",
    "\n",
    "The string type of the token is inconvenient to be used by models, which take numerical inputs. Now let's build a dictionary, often called vocabulary as well, to map string tokens into numerical indices starting from 0. To do so, we first count the unique tokens in all documents, called corpus, and then assign a numerical index to each unique token according to its frequency. Rarely appeared tokens are often removed to reduce the compexity. A token doesn't exist in corpus or has been removed is mapped into a special unknown (\"unk\") token. We optionally add another special tokens: \"pad\" a token for padding, \"bos\" to represent the beginning for a sentence, and \"eos\" for the ending of a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab(object):\n",
    "    def __init__(self, tokens, min_freq=0, use_special_tokens=False):\n",
    "        # Sort according to frequencies\n",
    "        counter = # type your code here\n",
    "        self.token_freqs = # type your code here\n",
    "        # type your code here\n",
    "        if use_special_tokens:\n",
    "            # padding, begin of sentence, end of sentence, unknown\n",
    "            # type your code here\n",
    "        else:\n",
    "            self.unk, uniq_tokens = # type your code here\n",
    "        uniq_tokens += # type your code here\n",
    "        self.idx_to_token, self.token_to_idx = [], dict()\n",
    "        for token in uniq_tokens:\n",
    "            # type your code here\n",
    "\n",
    "    def __len__(self):\n",
    "        return # type your code here\n",
    "    \n",
    "    def __getitem__(self, tokens):\n",
    "        if not isinstance(tokens, (list, tuple)):\n",
    "            return # type your code here\n",
    "        return # type your code here\n",
    "    def to_tokens(self, indices):\n",
    "        if not isinstance(indices, (list, tuple)):\n",
    "            return # type your code here\n",
    "        return # type your code here\n",
    "    \n",
    "def count_corpus(sentences):\n",
    "    # Flatten a list of token lists into a list of tokens\n",
    "    tokens = # type your code here\n",
    "    return # type your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We construct a vocabulary with the time machine dataset as the corpus, and then print the map between a few tokens to indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that, we can convert each sentence into a list of numerical indices. To illustrate things we print two sentences with their corresponding indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Put All Things Together\n",
    "\n",
    "We packaged the above code in the ``load_corpus_time_machine`` function, which returns ``corpus``, a list of token indices, and ``vocab``, the vocabulary. The modification we did here is that ``corpus`` is a single list, not a list of token lists, since we do not use the sequence information in the following models. Besides, we use character tokens to simplify the training in later sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_corpus_time_machine(max_tokens=-1):\n",
    "    lines = # type your code here\n",
    "    tokens = # type your code here\n",
    "    vocab = # type your code here\n",
    "    corpus = # type your code here\n",
    "    if max_tokens > 0:\n",
    "        corpus = # type your code here\n",
    "    return # type your code here\n",
    "\n",
    "corpus, vocab = load_corpus_time_machine()\n",
    "len(corpus), len(vocab)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
