{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Data Preparation\n",
    "\n",
    "Before introducing the model, let’s assume we will use a neural network to train a language model.Now the question is how to read mini-batches of examples and labels at random. We describe how to accomplish this for both random\n",
    "sampling and sequential partitioning strategies below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Sampling\n",
    "\n",
    "The following code randomly generates a minibatch from the data each time. Here, the batch size batch_size\n",
    "indicates to the number of examples in each mini-batch and num_steps is the length of the sequence (or time\n",
    "steps if we have a time series) included in each example. In random sampling, each example is a sequence\n",
    "arbitrarily captured on the original sequence. The positions of two adjacent random mini-batches on the\n",
    "original sequence are not necessarily adjacent. The target is to predict the next character based on what\n",
    "we’ve seen so far, hence the labels are the original sequence, shifted by one character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_data_iter_random(corpus, batch_size, num_steps):\n",
    "    # Offset the iterator over the data for uniform starts\n",
    "    # type your code here\n",
    "    # Subtract 1 extra since we need to account for label\n",
    "    num_examples = # type your code here\n",
    "    example_indices = # type your code here\n",
    "    random.shuffle(example_indices)\n",
    "    # This returns a sequence of the length num_steps starting from pos\n",
    "    data = # type your code here\n",
    "    # Discard half empty batches\n",
    "    num_batches = # type your code here\n",
    "    for i in range(0, batch_size * num_batches, batch_size):\n",
    "        # Batch_size indicates the random examples read each time\n",
    "        batch_indices = # type your code here\n",
    "        X = # type your code here\n",
    "        Y = # type your code here\n",
    "        yield # type your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us generate an artificial sequence from 0 to 30."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential Partitioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to random sampling of the original sequence, we can also make the positions of two adjacent\n",
    "random mini-batches adjacent in the original sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_data_iter_consecutive(corpus, batch_size, num_steps):\n",
    "    # Offset for the iterator over the data for uniform starts\n",
    "    # type your code here\n",
    "    # Slice out data - ignore num_steps and just wrap around\n",
    "    num_indices = # type your code here\n",
    "    Xs = # type your code here\n",
    "    Ys = # type your code here\n",
    "    Xs, Ys = # type your code here\n",
    "    num_batches = # type your code here\n",
    "    for i in range(0, num_batches * num_steps, num_steps):\n",
    "        X = # type your code here\n",
    "        Y = # type your code here\n",
    "        yield X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the same settings, print input X and label Y for each mini-batch of examples read by r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we wrap the above two sampling functions to a class so that we can use it as a normal pytorch data iterator later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqDataLoader(object):\n",
    "    \"\"\"A iterator to load sequence data\"\"\"\n",
    "    def __init__(self, batch_size, num_steps, use_random_iter, max_tokens):\n",
    "        if use_random_iter:\n",
    "            data_iter_fn = # type your code here\n",
    "        else:\n",
    "            data_iter_fn = # type your code here\n",
    "        self.corpus, self.vocab = # type your code here\n",
    "        self.get_iter = # type your code here\n",
    "    def __iter__(self):\n",
    "        return # type your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we define a function load_data_time_machine that returns both the data iterator and the vocabulary,\n",
    "so we can use it similarly as other functions with load_data prefix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_time_machine(batch_size, num_steps, \n",
    "                           use_random_iter=False, max_tokens=10000):\n",
    "    data_iter = # type your code here\n",
    "    return data_iter, data_iter.vocab"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
